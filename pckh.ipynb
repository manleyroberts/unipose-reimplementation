{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pckh.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "99hlTqgwOyo2"
      },
      "source": [
        "import numpy as np\n",
        "from gaussians import Gaussians\n",
        "import json\n",
        "import cv2\n",
        "import pickle\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-7Co0B_Pj6_"
      },
      "source": [
        "def calc_dists(preds, target):\n",
        "\t#preds  =  preds.astype(np.float32)\n",
        "\t#target = target.astype(np.float32)\n",
        "\t#dists  = np.zeros(preds.shape[0], preds.shape[1]))\n",
        "  #dists = np.linalg.norm(preds - target)\n",
        "  dists = torch.linalg.norm(preds - target)\n",
        "  #for n in range(preds.shape[0]):\n",
        "    #for c in range(preds.shape[1]):\n",
        "      #if target[n, c, 0] > 1 and target[n, c, 1] > 1:\n",
        "        #normed_preds   =  preds[n, c, :] / normalize[n]\n",
        "        #normed_targets = target[n, c, :] / normalize[n]\n",
        "        #dists[c, n]    = np.linalg.norm(preds - target)\n",
        "      #else:\n",
        "        #dists[c, n]    = -1\n",
        "  return dists"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASdjbVZGPnKE"
      },
      "source": [
        "def dist_acc(dists, threshold = 0.5):\n",
        "\tdist_cal     = np.not_equal(dists, -1)\n",
        "\tnum_dist_cal = dist_cal.sum()\n",
        "\n",
        "\tif num_dist_cal > 0:\n",
        "\t\treturn np.less(dists[dist_cal], threshold).sum() * 1.0 / num_dist_cal\n",
        "\telse:\n",
        "\t\treturn -1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEsA-nXHPpcA",
        "outputId": "bd9e5128-f1d2-445b-8a04-f1f001e2600e"
      },
      "source": [
        "with open('annotations/train.json') as f:\n",
        "\tdata = json.load(f)\n",
        "\n",
        "for i in range(len(data)):\n",
        "    if (data[i]['image'] == '000004812.jpg'):\n",
        "        img_name = data[i]['image']\n",
        "        #img = data[i]['image']\n",
        "        img = cv2.imread(img_name)\n",
        "        print(type(img))\n",
        "        kpt = np.asarray(data[i]['joints'], dtype=np.int32)\n",
        "        if img.shape[0] != 368 or img.shape[1] != 368:\n",
        "            kpt[:, 0] = kpt[:, 0] * (368 / img.shape[1])\n",
        "            kpt[:, 1] = kpt[:, 1] * (368 / img.shape[0])\n",
        "            img = cv2.resize(img, (368, 368))\n",
        "            img = np.array(img)\n",
        "        #print(kpt[0])\n",
        "        #print(kpt.shape)\n",
        "        gaussian = Gaussians()\n",
        "        g_list = []\n",
        "        g_list.append(kpt)\n",
        "        filename = 'epoch0_pickle'\n",
        "        device = \"cuda\"\n",
        "        heatmap = gaussian.expected_to_gaussian(g_list).to(device)\n",
        "        #print(heatmap.sum())\n",
        "        infile = open(filename,'rb')\n",
        "        model = pickle.load(infile)\n",
        "        device = \"cuda\"\n",
        "        img = torch.Tensor(img).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
        "        print(model(img).shape)\n",
        "        output = model(img)\n",
        "        softmax = nn.Softmax(dim=2)\n",
        "        # get Softmax over 2D image channels\n",
        "        input_view = output.view(output.shape[0], output.shape[1], -1)\n",
        "        output_view = softmax(input_view)\n",
        "        output = output_view.view(output.shape)\n",
        "        infile.close()\n",
        "        for j in range(16):\n",
        "          if(not (kpt[j, 0] < 0 or kpt[j, 1] < 0)):\n",
        "            dist = calc_dists(output[0, j], heatmap[0, j])\n",
        "            print(output[0, j].sum())\n",
        "            print(heatmap[0, j].sum())\n",
        "            print(dist)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "torch.Size([1, 16, 46, 46])\n",
            "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.3446, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.3430, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.2992, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.3056, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.3424, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.3441, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.3017, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.3027, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.3043, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.3003, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.2942, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.2943, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.2945, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.3016, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.2996, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', dtype=torch.float16)\n",
            "tensor(0.3009, device='cuda:0', grad_fn=<CopyBackwards>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}