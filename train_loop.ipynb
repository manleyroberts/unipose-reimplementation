{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7N92c2bvSSE"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://8080-5aed7051-8ac7-450f-8a6a-df73d6c6e7a2.cs-us-east1-omte.cloudshell.dev/"
    },
    "id": "ltAS_TXYvSSK",
    "outputId": "b3deed16-1987-4ae7-d6fa-63a988a147da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device: %s\" % device)\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ExUPuFeVvSSL"
   },
   "outputs": [],
   "source": [
    "with open('annotations/valid.json') as f:\n",
    "    test_data = json.load(f)\n",
    "with open('annotations/train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "storage_client = storage.Client(\"pose_estimation_2\")\n",
    "bucket = storage_client.get_bucket('pose_estimation_2_dataset_mpii')\n",
    "\n",
    "# NUM_TRAIN = 22246\n",
    "NUM_TRAIN = 128\n",
    "NUM_TEST = 2958"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBotjUmNvSSM"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OOFhpNErvSSM"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "epochs = 30\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRkFgyn-vSSM"
   },
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pltgIszsvSSM"
   },
   "outputs": [],
   "source": [
    "import modules\n",
    "import gc\n",
    "\n",
    "from modules.unipose import UniPose\n",
    "from modules.criterion.distribution_difference_loss import DistributionDifferenceLoss \n",
    "from modules.criterion.joint_max_mse_loss import JointMaxMSELoss\n",
    "\n",
    "model = UniPose().to(device)\n",
    "criterion = DistributionDifferenceLoss(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "alt_criterion = JointMaxMSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Load:26.04511260986328, Tensor Build:0.00010204315185546875\n",
      "Loaded 128 images, shape is torch.Size([128, 3, 720, 960])\n"
     ]
    }
   ],
   "source": [
    "load_start = time.time()\n",
    "\n",
    "# imagelist   = []\n",
    "# kptlist     = []\n",
    "\n",
    "torch_image = torch.zeros(NUM_TRAIN, 720, 960, 3)\n",
    "kpt_tensor = torch.zeros(NUM_TRAIN, 16, 2)\n",
    "\n",
    "# For each image, load the image\n",
    "for i in range(NUM_TRAIN):\n",
    "    img_name = train_data[i]['image']\n",
    "\n",
    "    blob = bucket.blob('MPII/images/' +  img_name)\n",
    "    blob.content_type = 'image/jpeg'\n",
    "    image = np.asarray(bytearray(blob.download_as_string()))\n",
    "    img = cv2.imdecode(image, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    kpt = np.asarray(train_data[i]['joints'], dtype=np.int32)\n",
    "\n",
    "    if img.shape[0] != 960 or img.shape[1] != 720:\n",
    "        kpt[:,0] = kpt[:,0] * (960/img.shape[1])\n",
    "        kpt[:,1] = kpt[:,1] * (720/img.shape[0])\n",
    "        img = cv2.resize(img,(960,720))\n",
    "        img = np.array(img)\n",
    "\n",
    "#     imagelist.append(img)\n",
    "#     kptlist.append(kpt)\n",
    "    torch_image[i,:,:,:] = torch.Tensor(img)\n",
    "    kpt_tensor[i,:,:]   = torch.Tensor(kpt)\n",
    "    \n",
    "#     if i % 1 == 0:\n",
    "#         print(f'Loaded {i+1} images')\n",
    "\n",
    "image_load_time = time.time()\n",
    "\n",
    "# construct image tensor and label tensor\n",
    "# torch_image = torch.Tensor(imagelist)\n",
    "torch_image = torch_image.permute(0, 3, 1, 2)\n",
    "# kpt_tensor = torch.FloatTensor(kptlist)\n",
    "\n",
    "tensor_build_time = time.time()\n",
    "\n",
    "print(f'Image Load:{image_load_time - load_start}, Tensor Build:{tensor_build_time - image_load_time}')\n",
    "print(f'Loaded {NUM_TRAIN} images, shape is {torch_image.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16, 2])\n",
      "torch.Size([128, 16, 90, 120])\n"
     ]
    }
   ],
   "source": [
    "import gaussians\n",
    "from gaussians import Gaussians\n",
    "\n",
    "print(kpt_tensor.shape)\n",
    "gaussian = Gaussians(device)\n",
    "expected_maps = gaussian.expected_to_gaussian(kpt_tensor)\n",
    "print(expected_maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://8080-5aed7051-8ac7-450f-8a6a-df73d6c6e7a2.cs-us-east1-omte.cloudshell.dev/"
    },
    "id": "kXks1wtZvSSN",
    "outputId": "01c17f06-4970-48c9-c86b-d330c47aba80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Batch Distribution Difference Loss: 0.9928291440010071\n",
      "Tensor Transfer:0.007822513580322266, Forward:0.4242668151855469, Criterion:0.0007879734039306641\n",
      "Backward:14.821150541305542, Optimizer_Step:6.055173635482788, Total:21.30920147895813\n",
      "Epoch: 0, Batch: 1, Batch Distribution Difference Loss: 0.9926469326019287\n",
      "Tensor Transfer:0.021384239196777344, Forward:0.1352851390838623, Criterion:0.0005202293395996094\n",
      "Backward:19.659369230270386, Optimizer_Step:1.20997953414917, Total:21.026538372039795\n",
      "Epoch: 0, Batch: 2, Batch Distribution Difference Loss: 0.9924332499504089\n",
      "Tensor Transfer:0.020699501037597656, Forward:0.1341719627380371, Criterion:0.0004553794860839844\n",
      "Backward:19.624255657196045, Optimizer_Step:1.2093572616577148, Total:20.98893976211548\n",
      "Epoch: 0, Batch: 3, Batch Distribution Difference Loss: 0.9923695921897888\n",
      "Tensor Transfer:0.02034926414489746, Forward:0.13541340827941895, Criterion:0.0004913806915283203\n",
      "Backward:19.647525548934937, Optimizer_Step:1.2085528373718262, Total:21.012332439422607\n",
      "Epoch: 0, Batch: 4, Batch Distribution Difference Loss: 0.9923153519630432\n",
      "Tensor Transfer:0.020764827728271484, Forward:0.13625550270080566, Criterion:0.00044345855712890625\n",
      "Backward:19.638524293899536, Optimizer_Step:1.2078452110290527, Total:21.003833293914795\n",
      "Epoch: 0, Batch: 5, Batch Distribution Difference Loss: 0.9936673641204834\n",
      "Tensor Transfer:0.021131515502929688, Forward:0.13583612442016602, Criterion:0.0004506111145019531\n",
      "Backward:19.632029056549072, Optimizer_Step:1.2089495658874512, Total:20.99839687347412\n",
      "Epoch: 0, Batch: 6, Batch Distribution Difference Loss: 0.9928863644599915\n",
      "Tensor Transfer:0.020282983779907227, Forward:0.13914895057678223, Criterion:0.0004763603210449219\n",
      "Backward:19.637146949768066, Optimizer_Step:1.2103033065795898, Total:21.00735855102539\n",
      "Epoch: 0, Batch: 7, Batch Distribution Difference Loss: 0.9928466081619263\n",
      "Tensor Transfer:0.020734310150146484, Forward:0.13447284698486328, Criterion:0.0004744529724121094\n",
      "Backward:19.651057958602905, Optimizer_Step:1.209172010421753, Total:21.01591157913208\n",
      "Epoch: 0, Batch: 8, Batch Distribution Difference Loss: 0.9934391379356384\n",
      "Tensor Transfer:0.021233320236206055, Forward:0.136582612991333, Criterion:0.0005297660827636719\n",
      "Backward:19.66804265975952, Optimizer_Step:1.2126469612121582, Total:21.039035320281982\n",
      "Epoch: 0, Batch: 9, Batch Distribution Difference Loss: 0.9924692511558533\n",
      "Tensor Transfer:0.020812511444091797, Forward:0.13457989692687988, Criterion:0.00045037269592285156\n",
      "Backward:19.644525289535522, Optimizer_Step:1.2098801136016846, Total:21.0102481842041\n",
      "Epoch: 0, Batch: 10, Batch Distribution Difference Loss: 0.9921460747718811\n",
      "Tensor Transfer:0.020818710327148438, Forward:0.13628768920898438, Criterion:0.0005218982696533203\n",
      "Backward:19.639832973480225, Optimizer_Step:1.2091288566589355, Total:21.006590127944946\n",
      "Epoch: 0, Batch: 11, Batch Distribution Difference Loss: 0.993069589138031\n",
      "Tensor Transfer:0.020830869674682617, Forward:0.13545751571655273, Criterion:0.0004591941833496094\n",
      "Backward:19.631427526474, Optimizer_Step:1.2128353118896484, Total:21.001010417938232\n",
      "Epoch: 0, Batch: 12, Batch Distribution Difference Loss: 0.9921460747718811\n",
      "Tensor Transfer:0.020217418670654297, Forward:0.13463521003723145, Criterion:0.0004942417144775391\n",
      "Backward:19.62353754043579, Optimizer_Step:1.209839105606079, Total:20.988723516464233\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "\n",
    "    # For each batch\n",
    "    for start_i in range(0, NUM_TRAIN, batch_size):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        torch_image_batch = torch_image[ start_i: start_i + batch_size ,:,:,:].to(device)\n",
    "        map_batch         = expected_maps[ start_i: start_i + batch_size ,:,:]\n",
    "\n",
    "        tensor_transfer_time = time.time()\n",
    "        \n",
    "        # Train on batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(torch_image_batch)\n",
    "        \n",
    "        forward_pass_time = time.time()\n",
    "        \n",
    "        batch_loss = criterion(out, map_batch)\n",
    "        \n",
    "        loss_function_time = time.time()\n",
    "        \n",
    "        epoch_loss.append(batch_loss.item())\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        backprop_time = time.time()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer_time = time.time()\n",
    "\n",
    "#         print(f'Epoch: {epoch}, Batch: {start_i // batch_size}, Batch Distribution Difference Loss: {batch_loss}, JointMaxMSELoss (to see if model is working): {alt_criterion(out, kpt_batch.to(device))}')\n",
    "        print(f'Epoch: {epoch}, Batch: {start_i // batch_size}, Batch Distribution Difference Loss: {batch_loss}')\n",
    "        print(f'Tensor Transfer:{tensor_transfer_time - start_time}, Forward:{forward_pass_time - tensor_transfer_time}, Criterion:{loss_function_time - forward_pass_time}')\n",
    "        print(f'Backward:{backprop_time - loss_function_time}, Optimizer_Step:{optimizer_time - backprop_time}, Total:{optimizer_time - start_time}')\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print(f'Epoch: {epoch}, Average Batch Loss: {sum(epoch_loss) / len(epoch_loss)}')\n",
    "    epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_loop.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "pytorch-gpu.1-8.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
